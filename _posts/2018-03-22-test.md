# 线性回归算法
用于解决回归问题
## 优点
- 思想简单，实现容易
- 许多强大的非线性模型的基础
从某种程度来说，以下都是线性回归的拓展：
    - 多项式回归
    - 逻辑回归
    - SVM
- 结果具有很好的可解释性
    - 线性回归的系数
        - 系数为正，正相关
        - 系数为负，负相关
        - 绝对值越大，影响程度越大
        - 因此，拿到一组数据，先试试用线性回归法试试，总归没有坏处
- 蕴含机器学习中的很多重要思想

## 缺点

## 简单线性回归
样本特征只有一个，称为：简单线性回归

1. 我们要寻找一个直线，最大程度的拟合样本的特征点。
2. 假设我们找到了最佳拟合方程`$y = ax + b$`
对于每一个样本特征，它就对应一个`$x^{(i)}$`(第i个样本对应的数据点)，它的输出标记就是`$y^{(i)}$`
3. 把`$x^{(i)}$`代入到式子`$y = ax + b$`，则有`$\hat{y}^{(i)} = ax^{(i)} + b$`，其中`$\hat{y}^{(i)}$`是根据方程计算出来的预测值，真值实际是`$y^{(i)}$`
4. 我们希望`$\hat{y}^{(i)}$`和`$y^{(i)}$`的差距尽量小，那么表达`$\hat{y}^{(i)}$`和`$y^{(i)}$`的差距：`$y^{(i)} - \hat{y}^{(i)}$`，但是差距应该是正数，那么则有：
    - 取绝对值：`$|y^{(i)} - \hat{y}^{(i)}|$`，但由于`$|y^{(i)} - \hat{y}^{(i)}|$`这个函数不是处处可导（为0处不可导），会影响求函数的极值，所以不选它
    - 取平方：`$(y^{(i)} - \hat{y}^{(i)})^2$`

考虑所有样本：`$\sum\limits_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2$`，因此我们的目标是：使`$\sum\limits_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2$`尽可能小
5. 把`$\hat{y}^{(i)} = ax^{(i)} + b$`代入，得`$\sum\limits_{i=1}^m(y^{(i)} - ax^{(i)} - b)^2$`
6. 目标，找到a和b，使得`$\sum\limits_{i=1}^m(y^{(i)} - ax^{(i)} - b)^2$`尽可能小，`$\sum\limits_{i=1}^m(y^{(i)} - ax^{(i)} - b)^2$`也可以称为损失函数（loss function）
7. 找到a和b，使得`$\sum\limits_{i=1}^m(y^{(i)} - ax^{(i)} - b)^2$`尽可能小，是典型的最小二乘法问题：最小化误差的平方
8. 可以直接得出a和b的数学解（`$\bar{x}$`，`$\bar{y}$`表示x，y的均值）：
- **`$a = \frac{\sum\limits_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum\limits_{i=1}^m(x^{(i)} - \bar{x})^2}$`**
- `$b = \bar{y} - a\bar{x}$`

### 手写实现简单线性回归
模型无需记录数据集，记录的只是模型的参数

### 衡量线性回归法的指标
对于简单线性回归的目标是找到a和b，使得`$\sum\limits_{i=1}^m(y^{(i)} - ax^{(i)} - b)^2$`尽可能小，越小代表效果越好。

那么把测试集的x和y代入，则有`$\hat{y}_{test}^{(i)} = ax_{test}^{(i)} + b$`，衡量它效果好坏的标准自然是`$\sum\limits_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2$`

**但是，这个标准有一个问题：结果与m的数目相关，等于说测试样本数约多，效果越差**，因此需要对标准略作变化
- 把结果除以样本数m： **均方误差MSE （Mean Squared Error）**
`$\frac{1}{m}\sum\limits_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2$`
- MSE会存在量纲不一致的问题（例如“米”和“ 米的平方”），可能会带来麻烦，所以把MSE再开平方根：**均方根误差 RMSE（Root Mean Squared Error）**
`$\sqrt{\frac{1}{m}\sum\limits_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2} = \sqrt{{MSE}_{test}}$`
- 由于是评估实际效果，无需求求函数的极值，因此衡量的指标也可以是：平均绝对误差 MAE （Mean Absolute Error）
`$\frac{1}{m}\sum\limits_{i=1}^m|y_{test}^{(i)} - \hat{y}_{test}^{(i)}|$`

这3个之中，通常来说，选择RMSE，但其实还有个更好的：
#### 评价回归算法 R Square
- MSE，RMSE，MAE存在问题：数值可以无限大
    - 假设算法用在评估房价时，误差是50000元，用在评估学生成绩时，误差是50分，那是否可以说明用在评估学生成绩时比用在评估房价时好呢？

对于分类的准确度：1最好，0最差，结果都在0~1之间，方便度量和比较
- 因此有公式：
**`$R^2 = 1 - \frac{SS_{residual}}{SS_{total}} = 1 - \frac{\sum\limits_{i}^m(\hat{y}^{(i)} - y^{(i)})^2}{\sum\limits_{i}^m(\bar{y} - y^{(i)})^2}$`**
    - 分子部分：`$\sum\limits_{i}^m(\hat{y}^{(i)} - y^{(i)})^2$`，表示使用我们的模型预测产生的错误。
    - 分母部分：`$\sum\limits_{i}^m(\bar{y} - y^{(i)})^2$`，表示使用y=`$\bar{y}$`预测产生的错误。（Baseline Model）理论上说，Baseline Model应该是会产生相当多的错误，因为它是基本的，生硬的把均值套上去而不是根据训练的模型评估出来的结果。
- 对于这个`$R^2$`：
    - `$R^2$` <= 1
    - `$R^2$` 越大越好。当我们的预测模型不翻任何错误时，`$R^2$` 得到最大值 1 - 0 = 1
    - 当我们的模型等于基准模型时，`$R^2$` 为0
    - 如果`$R^2$` < 0，说明我们学习到的模型还不如基准模型。此时，很可能我们的数据不存在任何线性关系。
- 把公式再稍作变化，把`$\frac{\sum\limits_{i}^m(\hat{y}^{(i)} - y^{(i)})^2}{\sum\limits_{i}^m(\bar{y} - y^{(i)})^2}$`的分子分母同时除以m，得到`$R^2 = 1 - \frac{\frac{1}{m}\sum\limits_{i}^m(\hat{y}^{(i)} - y^{(i)})^2}{\frac{1}{m}\sum\limits_{i}^m(\bar{y} - y^{(i)})^2} = 1 - \frac{MSE(\hat{y},y)}{Var(y)}$`，（MSE为均方误差，Var为方差）

## 多元线性回归
<a href="https://www.codecogs.com/eqnedit.php?latex=x^{(i)}&space;=&space;(X_1^{(i)},&space;X_2^{(i)},&space;...,&space;X_n^{(i)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{(i)}&space;=&space;(X_1^{(i)},&space;X_2^{(i)},&space;...,&space;X_n^{(i)})" title="x^{(i)} = (X_1^{(i)}, X_2^{(i)}, ..., X_n^{(i)})" /></a>
<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\theta_0&space;&plus;&space;\theta_1x_1&space;&plus;&space;\theta_2x_2&space;&plus;&space;...&space;&plus;&space;\theta_nx_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\theta_0&space;&plus;&space;\theta_1x_1&space;&plus;&space;\theta_2x_2&space;&plus;&space;...&space;&plus;&space;\theta_nx_n" title="y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n" /></a>
<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}^{(i)}&space;=&space;\theta_0&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}^{(i)}&space;=&space;\theta_0&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)}" title="\hat{y}^{(i)} = \theta_0 + \theta_1X_1^{(i)} + \theta_2X_2^{(i)} + ... + \theta_nX_n^{(i)}" /></a>

1. 我们知道简单线性回归的目标是找到a和b，使得<a href="https://www.codecogs.com/eqnedit.php?latex=\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2" title="\sum\limits_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2" /></a>尽可能小。
2. 那么推导到多元线性回归，我们的目标就是找到<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0,\theta_1,\theta_2,...,\theta_n," target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0,\theta_1,\theta_2,...,\theta_n," title="\theta_0,\theta_1,\theta_2,...,\theta_n," /></a>，使得<a href="https://www.codecogs.com/eqnedit.php?latex=\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2" title="\sum\limits_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2" /></a>尽可能小。
3. 现在看这个式子: <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}^{(i)}&space;=&space;\theta_0&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}^{(i)}&space;=&space;\theta_0&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)}" title="\hat{y}^{(i)} = \theta_0 + \theta_1X_1^{(i)} + \theta_2X_2^{(i)} + ... + \theta_nX_n^{(i)}" /></a>，我们最终要学习的参数可以整理出来：
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=&space;(\theta_0,&space;\theta_1,&space;\theta_2,...,\theta_n)^T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=&space;(\theta_0,&space;\theta_1,&space;\theta_2,...,\theta_n)^T" title="\theta = (\theta_0, \theta_1, \theta_2,...,\theta_n)^T" /></a>，<a href="https://www.codecogs.com/eqnedit.php?latex=\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" /></a>用列向量的方式表示。
4. 然后为了让式子更加工整，我们给<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0" title="\theta_0" /></a>乘以一个<a href="https://www.codecogs.com/eqnedit.php?latex=X_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_0" title="X_0" /></a>（设为常量1），于是式子变为：<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}^{(i)}&space;=&space;\theta_0X_0^{(i)}&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)},&space;X_0^{(i)}\equiv1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}^{(i)}&space;=&space;\theta_0X_0^{(i)}&space;&plus;&space;\theta_1X_1^{(i)}&space;&plus;&space;\theta_2X_2^{(i)}&space;&plus;&space;...&space;&plus;&space;\theta_nX_n^{(i)},&space;X_0^{(i)}\equiv1" title="\hat{y}^{(i)} = \theta_0X_0^{(i)} + \theta_1X_1^{(i)} + \theta_2X_2^{(i)} + ... + \theta_nX_n^{(i)}, X_0^{(i)}\equiv1" /></a>。
5. 于是<a href="https://www.codecogs.com/eqnedit.php?latex=X^{(i)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X^{(i)}" title="X^{(i)}" /></a>可以看成行向量<a href="https://www.codecogs.com/eqnedit.php?latex=(X_0^{(i)},&space;X_1^{(i)},&space;X_2^{(i)},...,X_n^{(i)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(X_0^{(i)},&space;X_1^{(i)},&space;X_2^{(i)},...,X_n^{(i)})" title="(X_0^{(i)}, X_1^{(i)}, X_2^{(i)},...,X_n^{(i)})" /></a>
6. <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}^{(i)}&space;=&space;X^{(i)}\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}^{(i)}&space;=&space;X^{(i)}\theta" title="\hat{y}^{(i)} = X^{(i)}\theta" /></a>
7. 结合数据所有样本，我们有：
<a href="https://www.codecogs.com/eqnedit.php?latex=X_b&space;=&space;\begin{pmatrix}1&X_1^{(1)}&X_2^{(1)}&...&X_n^{(1)}\\1&X_1^{(2)}&X_2^{(2)}&...&X_n^{(2)}\\...&...&...&...\\1&X_1^{(m)}&X_2^{(m)}&...&X_n^{(m)}\end{pmatrix}&space;\theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_b&space;=&space;\begin{pmatrix}1&X_1^{(1)}&X_2^{(1)}&...&X_n^{(1)}\\1&X_1^{(2)}&X_2^{(2)}&...&X_n^{(2)}\\...&...&...&...\\1&X_1^{(m)}&X_2^{(m)}&...&X_n^{(m)}\end{pmatrix}&space;\theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" title="X_b = \begin{pmatrix}1&X_1^{(1)}&X_2^{(1)}&...&X_n^{(1)}\\1&X_1^{(2)}&X_2^{(2)}&...&X_n^{(2)}\\...&...&...&...\\1&X_1^{(m)}&X_2^{(m)}&...&X_n^{(m)}\end{pmatrix} \theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" /></a>，因此有列向量
<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}&space;=&space;X_b&space;\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;X_b&space;\theta" title="\hat{y} = X_b \theta" /></a>
8. 我们把目标<a href="https://www.codecogs.com/eqnedit.php?latex=\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2&space;\Longrightarrow&space;(y^{(1)}&space;-&space;\hat{y}^{(1)})(y^{(1)}&space;-&space;\hat{y}^{(1)})&space;&plus;&space;(y^{(2)}&space;-&space;\hat{y}^{(2)})(y^{(2)}&space;-&space;\hat{y}^{(2)})&space;&plus;&space;...&space;&plus;&space;(y^{(m)}&space;-&space;\hat{y}^{(m)})(y^{(m)}&space;-&space;\hat{y}^{(m)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum\limits_{i=1}^m(y^{(i)}&space;-&space;\hat{y}^{(i)})^2&space;\Longrightarrow&space;(y^{(1)}&space;-&space;\hat{y}^{(1)})(y^{(1)}&space;-&space;\hat{y}^{(1)})&space;&plus;&space;(y^{(2)}&space;-&space;\hat{y}^{(2)})(y^{(2)}&space;-&space;\hat{y}^{(2)})&space;&plus;&space;...&space;&plus;&space;(y^{(m)}&space;-&space;\hat{y}^{(m)})(y^{(m)}&space;-&space;\hat{y}^{(m)})" title="\sum\limits_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2 \Longrightarrow (y^{(1)} - \hat{y}^{(1)})(y^{(1)} - \hat{y}^{(1)}) + (y^{(2)} - \hat{y}^{(2)})(y^{(2)} - \hat{y}^{(2)}) + ... + (y^{(m)} - \hat{y}^{(m)})(y^{(m)} - \hat{y}^{(m)})" /></a>可以看做是两个向量<a href="https://www.codecogs.com/eqnedit.php?latex=y-\hat{y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y-\hat{y}" title="y-\hat{y}" /></a>与<a href="https://www.codecogs.com/eqnedit.php?latex=y-\hat{y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y-\hat{y}" title="y-\hat{y}" /></a>的点乘，再把<a href="https://www.codecogs.com/eqnedit.php?latex=X_b&space;\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_b&space;\theta" title="X_b \theta" /></a>代入到<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}&space;=&space;X_b&space;\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;X_b&space;\theta" title="\hat{y} = X_b \theta" /></a>
9. 我们的目标变为寻找<a href="https://www.codecogs.com/eqnedit.php?latex=\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" /></a>，使得<a href="https://www.codecogs.com/eqnedit.php?latex=(y&space;-&space;X_b\theta)^T(y&space;-&space;X_b\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(y&space;-&space;X_b\theta)^T(y&space;-&space;X_b\theta)" title="(y - X_b\theta)^T(y - X_b\theta)" /></a>尽可能小
10. 由于接下来的推导过程对于大学教程来说是超纲的，所以直接给出结果：<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=&space;(X_b^TX_b)^{-1}X_b^Ty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=&space;(X_b^TX_b)^{-1}X_b^Ty" title="\theta = (X_b^TX_b)^{-1}X_b^Ty" /></a>
**注意不必过于执着这个值如何得出，以后我们还可以用梯度下降等方法呢。而且不理解推导过程也不影响最终使用**

### 多元线性回归的正规方程解（Normal Equation）
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=&space;(X_b^TX_b)^{-1}X_b^Ty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=&space;(X_b^TX_b)^{-1}X_b^Ty" title="\theta = (X_b^TX_b)^{-1}X_b^Ty" /></a>

- 问题：时间复杂度高。O(n^3) （优化后O(n^2.4)，仍然是高）
- 优点：不需要对数据进行归一化处理
    - 因为每个特征都有自己独立的特征系数，不存在量纲问题

注意：<a href="https://www.codecogs.com/eqnedit.php?latex=\theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" title="\theta=\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{pmatrix}" /></a>中，<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0" title="\theta_0" /></a>是截距（intercept），<a href="https://www.codecogs.com/eqnedit.php?latex=(\theta_1,&space;\theta_2,...\theta_n)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\theta_1,&space;\theta_2,...\theta_n)" title="(\theta_1, \theta_2,...\theta_n)" /></a>是系数（coefficients）。其中截距与特征是不相干的，对特征没有任何影响。有时我们要把截距和系数分开返回
- scikit-learn也是把系数和截距分开的

## 更多关于线性回归模型的讨论
- 线性回归具有强可解释性，是一个白盒子算法
    - 我们可以从系数中获得相关知识
- 只能解决回归问题
    - 虽然很多分类中，线性回归是基础
    - 对比kNN：kNN既可以解决分类，又可以解决回归
- 对数据有假设
    - 假设数据有线性关系
    - 对比kNN：kNN对数据没有假设
